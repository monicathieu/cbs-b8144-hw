---
title: "Assignment 5 EXAMPLE SOLUTION"
author: "For teaching team ONLY"
date: "10/6/2021"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      error = FALSE)
## load packages in this chunk here using library() 
## Best to load ALL necessary packages in this chunk
## so they are all loaded at the beginning
require(tidyverse)
require(tidymodels)
##
```

## Intro

Submit your answers as a knitted HTML document with a floating table of contents, showing both code and results. Each question should be its own section. This document has the output format pre-set, so you should get a correctly formatted output document by knitting this R Markdown file. **Turn in the HTML file on Canvas, NOT the R Markdown document!**

Improperly formatted submissions (submitting an Rmd instead of an HTML, or an HTML with a code error such that the document does not render properly, a document where code is not visible) will receive a 50% grade deduction.

Questions where your code returns an error will receive a 50% deduction. Please try your best to make sure your code *runs,* even if you're not sure if the output is correct!

If one chunk is giving you an error and stopping the document from rendering, but the rest of your code works and you need to knit, you can change `error = FALSE` in the setup chunk to `error = TRUE` so that error-producing code chunks will not stop the HTML document from rendering. We recommend not changing this setting unless you really need to render the document with errors included. Most of the time, it's helpful that the HTML document will not render when your code produces an error, because that helps you go back and fix your code.

## Outline (delete me)

Ridge regression, no CV, no tuning: coefplot
Lasso regression, no CV, tune penalty parameter, interpret penalty/lambda
Elastic net regression, no CV, tune mixture parameter
Logistic regression, CV


## Question 1

Read in data from the American Community Survey (ACS), stored at https://www.jaredlander.com/data/acs_ny.csv .

```{r q1-0-load-data}
# Code that loads data here
acs <- read.csv("https://www.jaredlander.com/data/acs_ny.csv",
                  header = TRUE) %>%
  as_tibble()
```

Estimate a **ridge** model of `FamilyIncome` from the ACS data.

### 1.1

_Split the data:_

```{r q1-1-split-data}
split_q1 <- initial_split(acs, prop = 0.8)
train_q1 <- training(split_q1)
test_q1 <- testing(split_q1)
```


_Prep the data:_ Use functions in the `recipes` package to prep your data.

- Omit `FoodStamp` as a predictor so that it is absent from the model.
- Set all nominal predictors to be turned into dummy variables.

```{r q1-1-prep-data}
# Code that preps data here
recipe_q1 <- recipe(FamilyIncome ~ ., data = train_q1) %>% 
  step_rm(FoodStamp) %>% 
  step_dummy(all_nominal(), one_hot = TRUE) %>% 
  step_normalize(all_numeric_predictors())
```

_Fit the model:_ Fit the model using functions from the `tidymodels` packages. Set `glmnet` as your model engine. Set `mixture` to the correct value to run a pure ridge regression.

```{r q1-1-set-model}
# Code that fits model here
model_q1 <- linear_reg(penalty = 0, mixture = 1) %>% 
  set_engine("glmnet")
```

```{r q-1-1-set-workflow}
workflow_q1 <- workflow() %>% 
  add_recipe(recipe_q1) %>% 
  add_model(model_q1)
```

```{r}
fit_train_q1 <- fit(workflow_q1, data = train_q1)
```

### 1.2

_Prepare to explore the model:_ Extract the underlying model object.

```{r}
glmnet_q1 <- extract_fit_engine(fit_train_q1)
```

_Explore the model:_ Plot a coefficient plot of the model, with the coefficients sorted by magnitude.

```{r}
coefplot::coefplot(glmnet_q1,
                   sort = "magnitude")
```

Write 2-3 sentences identifying the 3 strongest predictors (excluding the intercept) and interpreting why those predictors might predict family income.

## Question 2

Estimate a **lasso** model of `FamilyIncome` in the same ACS dataset, this time using _tuning_ and _cross-validation_..

_Prep the data:_ Use the train/test split and recipe that were prepped for Question 1. You do not need to re-prep the data.

```{r}
cv_train_q2 <- vfold_cv(train_q1, v = 8)
```

### 2.1

_Fit the model:_ Fit the model using functions from the `tidymodels` packages. Set `glmnet` as your model engine, with the correct setting of the `mixture` parameter to run _lasso_ regression instead of _ridge_ regression.

- Set the `penalty` parameter to be tuned using `tune()` to find the optimal penalty value for model complexity.
- Tune using a random grid of 50 possible parameter values of `penalty` between 0 and 10.
- Use `rmse` and `mae` to tune parameter values based on which one maximizes either root-mean-squared error or median absolute error.
- Set `glmnet` as your model engine.

```{r q2-1-set-model}
# Code that sets model here
model_q2 <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

```{r q2-1-set-tuning-grid}
tuning_grid_q2 <- model_q2 %>% 
  parameters() %>% 
  grid_random(size = 50)
```

```{r q2-1-set-tuning-metrics}
tuning_metrics_q2 <- metric_set(rmse,
                                mae)
```

```{r q-2-1-set-workflow}
workflow_q2 <- workflow() %>% 
  add_recipe(recipe_q1) %>% 
  add_model(model_q2)
```

```{r q2-1-fit-model-grid}
fit_q2 <- tune_grid(workflow_q2,
                    resamples = cv_train_q2,
                    grid = tuning_grid_q2,
                    metrics = tuning_metrics_q2)
```

### 2.2

Select the model with the "absolute best" penalty based on RMSE.

```{r q2-2-lambda.min-rmse}
# Code that outputs lambda.min here
fit_q2 %>% select_best(metric = "rmse")
```

Select the model with the "absolute best" penalty based on MAE.

```{r q2-2-lambda.min-mae}
# Code that outputs lambda.min here
fit_q2 %>% select_best(metric = "mae")
```
Show the coeï¬ƒcients for both models, both as coefficient plots.

```{r q1-4-coefplot-lambda.min}
# Code that outputs coefplot using lambda.min here

```

```{r q1-4-coefplot-lambda.1se}
# Code that outputs coefplot using lambda.1se here

```

### 2.3

Plot a multiplot of the coefficients of the final ridge model from Q1 and the best-by-RMSE lasso model from Q2. 

```{r}
glmnet_q2 <- workflow_q2 %>% 
  finalize_workflow(fit_q2 %>% select_best("rmse")) %>% 
  fit(data = train_q1) %>% 
  extract_fit_engine()
```

```{r}
coefplot::multiplot(glmnet_q1, glmnet_q2, sort = "magnitude")
```

Write 2-3 sentences explaining why the coefficients differ between these models.

**REPLACE ME WITH YOUR EXPLANATION OF THE RIDGE VS LASSO COEFFICIENTS**

## Question 3

Estimate an _elastic net_ model of `FamilyIncome` in the same ACS dataset, using _tuning_ and _cross-validation_. An elastic net model is a blend of a pure lasso and pure ridge model. In this question, you will tune the `mixture` parameter to adjust the amount of "lasso-ness" vs. "ridge-ness" in the model.

### 3.1

_Prep the data:_ Use the data that were prepped for Question 1, and the cross-validation spec for the training data prepped for Question 2.

_Fit the model:_ Fit the model using functions from the `tidymodels` packages.

- Set the both the `penalty` and `mixture` parameters to be tuned using `tune()`.
- Tune using a regular grid of 10 x 10 possible parameter combinations of `penalty` of `mixture`.
- Use `rmse` to tune parameter values based on which one minimizes root-mean-squared error.
- Set `glmnet` as your model engine.

```{r q3-1-set-model}
# Code that sets model here
model_q3 <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")
```

```{r q3-1-set-tuning-grid}
tuning_grid_q3 <- model_q3 %>% 
  parameters() %>% 
  grid_regular(levels = 10)
```

```{r q3-1-set-tuning-metrics}
tuning_metrics_q3 <- metric_set(rmse)
```

```{r q-3-1-set-workflow}
workflow_q3 <- workflow() %>% 
  add_recipe(recipe_q1) %>% 
  add_model(model_q3)
```

```{r q3-1-fit-model-grid}
fit_q3 <- tune_grid(workflow_q3,
                    resamples = cv_train_q2,
                    grid = tuning_grid_q3,
                    metrics = tuning_metrics_q3)
```

Show the best 5 models by RMSE.

```{r}
fit_q3 %>% 
  show_best("rmse")
```

### 3.2

_Prepare to explore the model:_ Extract the underlying model object.

Finalize the workflow using the best model by RMSE, fit the final model, and extract the resulting `glmnet` model object.

```{r}
glmnet_q3 <- workflow_q3 %>% 
  finalize_workflow(fit_q3 %>% select_best("rmse")) %>% 
  fit(data = train_q1) %>% 
  extract_fit_engine()
```

_Explore the model:_ Show a coefficient plot with the coefficients sorted by magnitude.

```{r}
glmnet_q3 %>% coefplot::coefplot(sort = "magnitude")
```
Write 2-3 sentences describing whether the model is more like a lasso or a ridge regression, and what information you used to arrive at that answer.

## Question 4

Download the 2015 NFL Play-by-Play data from https://www.jaredlander.com/data/pbp-2015.csv and fit a model to predict whether a given play is a rush or pass play (`PlayType`).

```{r}
nfl <- read_csv("https://www.jaredlander.com/data/pbp-2015.csv") %>% 
  select(-where(~all(is.na(.))))
```


### 4.1

Prep the data: Filter the data to include plays from just one `OffenseTeam`, where `PlayType` is either "RUSH" or "PASS".

```{r}
nfl <- nfl %>% 
  filter(OffenseTeam == "DAL", PlayType %in% c("RUSH", "PASS"))
```

Split the data into training and testing sets, with 75% of the data in the training set. Stratify the sets by `PlayType` so that the proportions are balanced in both sets.

Extract the training and testing data for later use.

```{r}
split_q4 <- initial_split(nfl, prop = 0.75, strata = "PlayType")
train_q4 <- training(split_q4)
test_q4 <- testing(split_q4)
```

Prepare a cross-validation spec using the training data with 10 folds, again stratifying each fold by `PlayType`.

```{r}
cv_q4 <- vfold_cv(train_q4, v = 10, strata = "PlayType")
```

Prepare a model recipe predicting whether `PlayType` is "PASS" or "RUSH" based on the following _specific_ predictors:

`Quarter`: Quarter of game (1-4)
`Minute`: Minutes remaining in the quarter (15-0)
`DefenseTeam`: The opposing team on defense
`Down`: 1st, 2nd, 3rd, or 4th
`ToGo`: Yards to go to make the down
`YardLineFixed`: Starting yard line for that down

Using `recipes` and `themis` steps:

- Change `PlayType` from string to factor to prepare for logistic regression
- Downsample the data by `PlayType`
- Dummy-code all nominal predictors, _not_ using one-hot encoding

```{r}
recipe_q4 <- recipe(PlayType ~ Quarter + Minute + DefenseTeam + Down + ToGo + YardLineFixed, data = train_q4) %>% 
  step_string2factor(PlayType) %>% 
  themis::step_downsample(PlayType) %>% 
  step_dummy(all_nominal_predictors())
```

### 4.2

_Fit the model:_ Set up the logistic regression model using functions from the `tidymodels` packages. Set `glmnet` as your model engine. Set the `penalty` to .01, and tune the `mixture` parameter to run an elastic net regression.

```{r}
model_q4 <- logistic_reg(penalty = .01,
                         mixture = tune()) %>% 
  set_engine("glmnet")
```

Set up a tuning grid to select 50 random values of `mixture`.

```{r q4-1-set-tuning-grid}
tuning_grid_q4 <- model_q4 %>% 
  parameters() %>% 
  grid_random(size = 50)
```

Set up tuning metrics using `mn_log_loss` to evaluate the models based on log-loss (lower is better!).

```{r q4-1-set-tuning-metrics}
tuning_metrics_q4 <- metric_set(mn_log_loss)
```

Prep the workflow.

```{r q-4-1-set-workflow}
workflow_q4 <- workflow() %>% 
  add_recipe(recipe_q4) %>% 
  add_model(model_q4)
```

Fit the tuning grid of models.

```{r q4-1-fit-model-grid}
fit_q4 <- tune_grid(workflow_q4,
                    resamples = cv_q4,
                    grid = tuning_grid_q4,
                    metrics = tuning_metrics_q4)
```

```{r q4-1-finalize-model}
final_fit_q4 <- workflow_q4 %>% 
  finalize_workflow(parameters = fit_q4 %>% select_best()) %>% 
  last_fit(split_q4)
```

### 4.3

Explore the model: Show a variable importance plot.

```{r}
final_fit_q4 %>% 
  extract_fit_engine() %>% 
  coefplot::coefplot(sort = "magnitude")
```

For the top three predictors, write 1-2 sentences each describing the predictor and interpreting the coefficient.

### 4.4

Report the model's rush/pass play classification accuracy using `collect_metrics()`.

```{r}
final_fit_q4 %>% 
  collect_metrics()
```

In 1-2 sentences, explain whether the model appears to be performing above or below chance, and how you can tell.

Plot an ROC curve of the model's performance on the held-out test data where the `truth` is `PlayType` (which contains the actual rush/pass play type), and where the class probability column is the predicted probability that the play is a PASS based on the model.

```{r}
final_fit_q4 %>% 
  collect_predictions() %>% 
  roc_curve(truth = PlayType, .pred_PASS) %>% 
  autoplot()
```

